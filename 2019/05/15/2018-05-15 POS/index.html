<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文主要介绍无监督贝叶斯pos tagging方法，实验结果和分析。简单概括最大似然估计(MLE)，隐藏马尔可夫模型(HMM)。 原文参考 A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging (Sharon Goldwater, Thomas L. Griffiths, 2016) 还是懒癌，暂时是英文的">
<meta property="og:type" content="article">
<meta property="og:title" content="Unsupervised PoS Tagging">
<meta property="og:url" content="http://yoursite.com/2019/05/15/2018-05-15%20POS/index.html">
<meta property="og:site_name" content="Shinoyuki">
<meta property="og:description" content="本文主要介绍无监督贝叶斯pos tagging方法，实验结果和分析。简单概括最大似然估计(MLE)，隐藏马尔可夫模型(HMM)。 原文参考 A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging (Sharon Goldwater, Thomas L. Griffiths, 2016) 还是懒癌，暂时是英文的">
<meta property="article:published_time" content="2019-05-14T16:00:00.000Z">
<meta property="article:modified_time" content="2020-04-26T13:28:11.582Z">
<meta property="article:author" content="Xiaoqing Tang">
<meta property="article:tag" content="ML">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2019/05/15/2018-05-15%20POS/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>Unsupervised PoS Tagging | Shinoyuki</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Shinoyuki</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/shinoyuki222/" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/15/2018-05-15%20POS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaoqing Tang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shinoyuki">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Unsupervised PoS Tagging
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-15 00:00:00" itemprop="dateCreated datePublished" datetime="2019-05-15T00:00:00+08:00">2019-05-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-26 21:28:11" itemprop="dateModified" datetime="2020-04-26T21:28:11+08:00">2020-04-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Natural-Language-Processing/" itemprop="url" rel="index"><span itemprop="name">Natural Language Processing</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文主要介绍无监督贝叶斯pos tagging方法，实验结果和分析。简单概括最大似然估计(MLE)，隐藏马尔可夫模型(HMM)。</p>
<p>原文参考 <a href="http://www.aclweb.org/anthology/P07-1094" target="_blank" rel="noopener">A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging (Sharon Goldwater, Thomas L. Griffiths, 2016)</a></p>
<p>还是懒癌，暂时是英文的</p>
<!-- 写中文的notes好累啊，所以这篇暂时是英文的 ( ´▽｀) -->
<a id="more"></a>
<h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><h4 id="Inference-for-HMMs"><a href="#Inference-for-HMMs" class="headerlink" title="Inference for HMMs"></a>Inference for HMMs</h4><ul>
<li>Notation: Given that $t_{i-1} = t$, the value of $t_i$ is drawn from a multinominal dstribution with parameters $\tau^{(t)}$<script type="math/tex; mode=display">
  \begin{eqnarray}
  t_i \mid t_{i-1}=t,\quad \tau^{(t)} \sim Multinominal (\tau^{(t)})\\
  w_i \mid t_{i}=t, \quad \omega^{(t)} \sim Multinominal (\omega^{(t)})
  \end{eqnarray}</script></li>
<li>Inference: decoding, applying the model at the test time, we need to know $\mathbf \theta$ and we can compute $P(\mathbf t, \mathbf w)$<script type="math/tex; mode=display">P(\mathbf t, \mathbf w) = \prod \limits_{i=1}^n P(t_i \mid t_{i-1})= \prod \limits_{i=1}^n \tau^ {(t_{i-1})} \omega_{w_i}^{t_i}</script></li>
<li>Compute $P(\mathbf w)$, such as language model<script type="math/tex; mode=display">P(\mathbf w) = \sum \limits _ {\mathbf t} P(\mathbf w, \mathbf t)</script></li>
<li>Also $P(\mathbf t \mid \mathbf w) $, such as PoS tagger<script type="math/tex; mode=display">P(\mathbf t \mid \mathbf w) = \frac{P(\mathbf t, \mathbf w)}{P(\mathbf w)}</script></li>
</ul>
<h4 id="Parameter-Estimation-for-HMMs"><a href="#Parameter-Estimation-for-HMMs" class="headerlink" title="Parameter Estimation for HMMs"></a>Parameter Estimation for HMMs</h4><ul>
<li>Estimation: training the model, determing its params. A procedure to set $\mathbf \theta$ based on data.</li>
<li>Bayes Rule<script type="math/tex; mode=display">P(\theta \mid w) = \frac{P(w \mid \theta)P(\theta)}{P(w)} \propto P(w \mid \theta)P(\theta)</script></li>
<li>Could use MLE, Bayesian estimation (actually no parameter estimation)</li>
</ul>
<h4 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h4><ul>
<li>Choose the $\mathbf \theta$ that makes the data most probable, ignore the prior term. Equivalent to assuming a uniform prior.<script type="math/tex; mode=display">\hat \theta = argmax _ \theta P(\mathbf w \mid \theta)</script></li>
<li>supervised systems, \textit{relative frequency estimate} is equivalent to the MLE.<script type="math/tex; mode=display">\tau_{t'}^{(t)} = \frac{n(t,t')}{n(t)} \quad \quad \omega_{w}^{(t)} = \frac{n(t,w)}{n(t)}</script></li>
<li>unsupervised systems, expectation maximization (EM) algorithm to estimate $\theta$</li>
<li>Process:<ul>
<li>E-step: use current estimate of $\theta$ to compute expected counts of hidden events $n$.</li>
<li>M-step: recompute $\theta$ using expected counts.s</li>
</ul>
</li>
<li>Examples: forward-backward algorithm for HMMs, inside-outside algorithm for PCFGs, k-meaning clustering.</li>
<li>EM Works well on : word alignment for machine translation; speech recognition….</li>
<li>Often <strong>fails</strong>:<ul>
<li>probabilistic context-free grammars (PCFG): highly sensitive to initialisation; F-score reported are generally low.</li>
<li>For HMMs, even very small amounts of training data have been show to work better than EM.</li>
<li>similar picture for many other tasks</li>
</ul>
</li>
</ul>
<h3 id="Main-Model"><a href="#Main-Model" class="headerlink" title="Main Model"></a>Main Model</h3><h4 id="Bayesian-HMM"><a href="#Bayesian-HMM" class="headerlink" title="Bayesian HMM"></a>Bayesian HMM</h4><ul>
<li>Parameter Estimation: we are not interested in the value of $\theta$<script type="math/tex; mode=display">
  \begin{eqnarray}
  P(w_i \mid \mathbf w_{1:i-1}) &=& \int P(w_i \mid \theta) P(\theta \mid \mathbf w_{1:i-1}) d \theta\\
  P(\mathbf t \mid w) &=& \int P(\mathbf t \mid \mathbf w,\theta) P(\theta \mid \mathbf w) d \theta
  \end{eqnarray}</script></li>
<li><p>Bayesian integration: Integrating over $\theta$ gives us an <em>average</em> over all possible parameters values.</p>
<ul>
<li>accounts for uncertainty as to the exact value of $\theta$</li>
<li>models the shape of the distribution over $\theta$</li>
<li>increase robustness: there may be a range of good values of $\theta$</li>
<li>we can use priors favouring sparse solutions.</li>
</ul>
</li>
<li><p>Model:</p>
<script type="math/tex; mode=display">
\begin{eqnarray}
t_i \mid t_{i-1}=t,\quad \tau^{(t)} &\sim& Multinominal (\tau^{(t)})\\
w_i \mid t_{i}=t,\quad \omega^{(t)} &\sim& Multinominal (\omega^{(t)})\\
\tau^{(t)} \mid \alpha &\sim& Dirichlet(\alpha)\\
\omega^{(t)} \mid \beta &\sim& Dirichlet(\beta)
\end{eqnarray}</script></li>
<li><p>intergrate out the parameters $\mathbf \theta = (\mathbf \tau, \mathbf \omega)$, we calculate probablity for each of $T$ possible tags</p>
<script type="math/tex; mode=display">
\begin{eqnarray}
P(t_i \mid \mathbf t_{1:i-1}; \alpha) &=& \frac{n(t_{i-1},t_i) + \alpha}{n(t_{i-1})+ T\alpha}\\
P(w_i \mid t_i, \mathbf t_{1:i-1}, \mathbf w; \beta) &=& \frac{n(t_{i},w_i) + \beta}{n(t_{i})+ W_{t_i}\beta}
\end{eqnarray}</script></li>
<li><p>For inference, $P(\mathbf t \mid \mathbf w)$ using an estimation method called \textit{Gibbs sampling}.</p>
</li>
</ul>
<h4 id="Dirichlet-Distribution"><a href="#Dirichlet-Distribution" class="headerlink" title="Dirichlet Distribution"></a>Dirichlet Distribution</h4><ul>
<li>a distribution over distribution, a prior.</li>
<li>Definition<script type="math/tex; mode=display">P(\theta) = \frac{1}{Z}\prod \limits _ {j=1}^ K \theta_j ^{\alpha_j -1}</script></li>
<li>$\alpha$ is params. of Dirichlet Distribution, we usually use symmetric Dirichlets, where $\alpha_1 … \alpha_K$ is equal to $\beta$. Denote Dirichlet($\beta$) to mean Dirichlet($\beta,…,\beta)$</li>
<li>property:<ul>
<li>With $\beta &gt;1$, we would \textbf{prefer} uniform distributions over $\theta$</li>
<li>With $\beta = 1$, we have no preference on $\theta$, <strong>does not </strong>mean we choose uniform.</li>
<li>With $\beta &lt; 1$, we prefer sparse (skewed) distributions</li>
</ul>
</li>
</ul>
<h4 id="Evaluation-of-BHMM"><a href="#Evaluation-of-BHMM" class="headerlink" title="Evaluation of BHMM"></a>Evaluation of BHMM</h4><ul>
<li><p>Compare with MLHMM and CRF/CE</p>
<ul>
<li><p>Results</p>
<ul>
<li>Intergrating over parameters is useful in itself, even with uninformative prior ($\alpha = \beta =1$):</li>
<li>Better prior can help even more, though do not reach the state of the art.</li>
</ul>
</li>
<li><p>The BHMM indentifies a sequence of tages that have high prob. over a range of parameter values, rather than choosing tags based on the single best set of paprameters (MLHMM).</p>
</li>
<li><p>The smaller effect of $\beta$: </p>
<p>although the true output distribution tend to be sparse as well, the level of sparseness depends on the tag (consider function words vs. content words in particular). Therefore, the value of $\beta$ that accurately reflects the most probable output distributions for some tags may be a poor choice for other tags.</p>
</li>
<li>The trasition probabilities matrix is sparse: the optimal value of $\alpha$ is 0.003.</li>
</ul>
</li>
<li>Sytactic clustering<ul>
<li>With MLHMM: different tokens of the same word type are usually assigned to the same cluster, but types are assigned to clusters more or less at random, and all clusters have approximately the same number of types.</li>
<li>BHMM: the clusters found by BHMM tend to be more coherent and more variable in size</li>
<li>BHMM transition matrix is sparse, MLHMM is not.</li>
</ul>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Unsupervised PoS tagging is useful to build lexica and taggers for new language or domains; </li>
<li>maximum likelihood HMM with EM performs poorly; (see MLE)</li>
<li>Bayesian HMM with Gibbs sampling can be used instead;<ul>
<li>no intereted in params, instead of parameter estimation, we do integration.</li>
<li>Gibbs Sampling (for inference, see below)</li>
</ul>
</li>
<li>the Bayesian HMM improves performance by averaging out uncertainty;<ul>
<li>MLE assume uniform values over $\mathbf \theta$, with $100\%$ certainty, and then pick one set of $\theta$ which maximum the liklihood.</li>
<li>Bayesian HMM does not make that assumption, and if we choose a certain hyperparameter $\alpha$ and $\beta$, we have different certainty of the set of $\theta$. <ul>
<li>With $\alpha,~ \beta &gt;1$, we would \textbf{prefer} uniform values over $\mathbf \theta$</li>
<li>With $\beta = 1$, we have no preference on $\mathbf \theta$, does not mean we choose uniform.</li>
<li>With $\beta &lt; 1$, we prefer sparse value of $\mathbf \theta$.</li>
</ul>
</li>
</ul>
</li>
<li>it also allows us to use priors that favour sparse solutions as they occur in language data.</li>
<li>Other types of discrete latent variable models (e.g. for syntax or semantics) use similar methods.</li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>Goldwater, S., &amp; Griffiths, T. (2007). A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th annual meeting of the association of computational linguistics (pp. 744-751).</p>
<center><font size=2 face="Aial" color=#8F8F8F>Modified 18th-May-2018 12:00</font></center>


    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ML/" rel="tag"># ML</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/05/12/2018-05-12%20RNNG/" rel="prev" title="Recurrent neural network grammars (RNNG)">
      <i class="fa fa-chevron-left"></i> Recurrent neural network grammars (RNNG)
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/05/18/2018-05-18%20Dependency%20Parsing/" rel="next" title="Dependency Parsing">
      Dependency Parsing <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Intro"><span class="nav-number">1.</span> <span class="nav-text">Intro</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Inference-for-HMMs"><span class="nav-number">1.1.</span> <span class="nav-text">Inference for HMMs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parameter-Estimation-for-HMMs"><span class="nav-number">1.2.</span> <span class="nav-text">Parameter Estimation for HMMs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Maximum-Likelihood-Estimation"><span class="nav-number">1.3.</span> <span class="nav-text">Maximum Likelihood Estimation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-Model"><span class="nav-number">2.</span> <span class="nav-text">Main Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bayesian-HMM"><span class="nav-number">2.1.</span> <span class="nav-text">Bayesian HMM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dirichlet-Distribution"><span class="nav-number">2.2.</span> <span class="nav-text">Dirichlet Distribution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluation-of-BHMM"><span class="nav-number">2.3.</span> <span class="nav-text">Evaluation of BHMM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">3.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Xiaoqing Tang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoqing Tang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>

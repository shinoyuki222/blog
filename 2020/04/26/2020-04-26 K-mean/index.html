<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="大家都知道K-mean是无监督学习的经典算法。那为什么K-mean是无监督学习呢？可能有人觉得：这也要证明？ 其实这是一个值得思考的问题……仔细思考这个问题，你会觉得自己似乎不太懂K-mean了？ 哈哈，不要多想，本文意在剖析K-mean算法的本质： K-mean算法是一种隐变量模型；隐变量模型可以用EM算法计算参数；无监督EM算法的其他应用。 本文单纯分享我自己的理解，如有不准确的地方欢迎讨论，">
<meta property="og:type" content="article">
<meta property="og:title" content="K-means and Latent variable model">
<meta property="og:url" content="http://yoursite.com/2020/04/26/2020-04-26%20K-mean/index.html">
<meta property="og:site_name" content="Shinoyuki">
<meta property="og:description" content="大家都知道K-mean是无监督学习的经典算法。那为什么K-mean是无监督学习呢？可能有人觉得：这也要证明？ 其实这是一个值得思考的问题……仔细思考这个问题，你会觉得自己似乎不太懂K-mean了？ 哈哈，不要多想，本文意在剖析K-mean算法的本质： K-mean算法是一种隐变量模型；隐变量模型可以用EM算法计算参数；无监督EM算法的其他应用。 本文单纯分享我自己的理解，如有不准确的地方欢迎讨论，">
<meta property="article:published_time" content="2020-04-25T16:00:00.000Z">
<meta property="article:modified_time" content="2020-04-26T13:33:35.301Z">
<meta property="article:author" content="Xiaoqing Tang">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2020/04/26/2020-04-26%20K-mean/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>K-means and Latent variable model | Shinoyuki</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Shinoyuki</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/shinoyuki222/" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/26/2020-04-26%20K-mean/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xiaoqing Tang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shinoyuki">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          K-means and Latent variable model
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-04-26 00:00:00 / Modified: 21:33:35" itemprop="dateCreated datePublished" datetime="2020-04-26T00:00:00+08:00">2020-04-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>大家都知道K-mean是无监督学习的经典算法。那为什么K-mean是无监督学习呢？可能有人觉得：这也要证明？</p>
<p>其实这是一个值得思考的问题……仔细思考这个问题，你会觉得自己似乎不太懂K-mean了？</p>
<p>哈哈，不要多想，本文意在剖析K-mean算法的本质：</p>
<p>K-mean算法是一种隐变量模型；隐变量模型可以用EM算法计算参数；无监督EM算法的其他应用。</p>
<p>本文单纯分享我自己的理解，如有不准确的地方欢迎讨论，互相进步～</p>
<a id="more"></a>
<h3 id="隐变量模型"><a href="#隐变量模型" class="headerlink" title="隐变量模型"></a>隐变量模型</h3><p>隐变量模型指的是：已知的可观测到变量$\mathbf x$可以由一个或多个未知变量$\mathbf z$以一定的方式生成，并且这个隐变量是我们假定的。可能有人觉得：为什么那么麻烦要定义隐变量增加模型复杂度呢？</p>
<p>我觉得隐变量模是有现实意义的。比如一张图片是由颜色，亮度，对比度，分辨率等等决定；一句话的意思是由文字，语气，标点决定；一个人是由DNA里的信息决定……这些实际例子可以说明隐变量模型的合理性。</p>
<p>比较经典的隐变量模型有：HMM, GAN, VAE, LDA…我就不多做赘述。聪明的同学应该发现这些模型都是生成式模型（Generative model）。如果不知道生成式模型是什么的同学可以自我拓展一下。</p>
<h3 id="K-mean"><a href="#K-mean" class="headerlink" title="K-mean"></a>K-mean</h3><p>K-mean算法很简单，就是给一堆数据$\mathbf x$，这些都是观测到的。训练好的模型将数据氛围K个子集，各个聚类子集内的所有数据样本的均值作为该聚类的代表点。预测一个新的数据$x^<em>$时，计算$x^</em>$ 和K个中心点的“距离”，选择“距离”最近的中心，并把$x^*$归为该中心所在的类簇。</p>
<p>这个算法的不太标准的比喻：模型把一堆图片分成了2类，我拿到一看，诶，模型真聪明正巧一堆是猫的图片，另一堆是狗的图片。然后我随机丢给模型一张小老虎的照片，模型把该图片和两堆图片比较了一下，觉得和 “猫” 的那堆图片相似度高。于是模型就把该图片分到了“猫”那一类。</p>
<p>这时候你要说了，这明明是老虎怎么能分给猫呢？但在2分类的情况下，分到猫并没有错误。更关键的是这里的“猫” 和 “狗”的定义是人给的。模型只是认为这张小老虎的照片和猫堆的照片是同一类。所以也许应该定义为“猫科”和“犬科”哈哈哈哈……</p>
<p>所以其实K-means算法有很多问题，但这不是我们今天的重点。</p>
<h3 id="K-mean算法是一种隐变量模型"><a href="#K-mean算法是一种隐变量模型" class="headerlink" title="K-mean算法是一种隐变量模型"></a>K-mean算法是一种隐变量模型</h3><p>那么为什么K-mean算法是一种隐变量模型呢？判断一个模型是不是隐变量模型，比较关键的一点是：找出隐变量。</p>
<p>所以问题就转化成了K-mean算法的隐变量是什么？是K吗？对了一半，我觉得是K个中心点。</p>
<p>也就是说K-mean算法认为：所有观测数据可以用K个中心点表达。比如刚才图片的例子中，我觉得所有图片就分成两堆。</p>
<h3 id="K-mean算法怎么求"><a href="#K-mean算法怎么求" class="headerlink" title="K-mean算法怎么求"></a>K-mean算法怎么求</h3><p>其实刚才举的猫狗图片例子中，我非常“狡猾”地直接跳过了训练的步骤，直接说模型把一堆图片分成了2类。所以模型是怎么把图片分成这样的两类呢？是不是有可能分成别的两类，比如浅色动物和深色动物？那要怎么样才能认为模型的分类是我们想要的呢？我们来探讨一下…</p>
<p>写的有点累了…反正没人看，先把标题写了</p>
<h3 id="K-mean算法的cost-function"><a href="#K-mean算法的cost-function" class="headerlink" title="K-mean算法的cost function"></a>K-mean算法的cost function</h3><h3 id="K-mean算法的求解过程，和隐变量模型的EM算法的关系。"><a href="#K-mean算法的求解过程，和隐变量模型的EM算法的关系。" class="headerlink" title="K-mean算法的求解过程，和隐变量模型的EM算法的关系。"></a>K-mean算法的求解过程，和隐变量模型的EM算法的关系。</h3><!-- 
#### Deep learning is not magic and will not solve all of your problems, but representation learning is a very powerful idea.
- NNs don't reuse other informations, only use the input info.
- Human beings often use other informations, we learn language to solve other problems.
- Word embedding: if we learn a matrix, can keep matrices to initialise some other problem.

#### Word representations can be transferred between models.
- word representations pick up all associations: some are useful to applications; some are harmful for people to building a system (e.g., bias problem).


\subsubsection*{Word2vec trains word representations using an objective based on language modeling--so it can be trained on unlabeled data.}

\begin{itemize}
  \item we could use POS to training the word representations: Feedforward is feasible, could conditioned on entire input sentence.
  \item using an objective based on language modeling, therefore, it can be trained on unlabeled data.
  \item training a LM is supervised learning problem: it predicts a word conditioned on the context, all you need for supervised learning are examples of context words

  \item idea of CBOW (continuous bag-of-words)
    \begin{itemize}
      \item CBOW adds inputs from words within short window to predict the current word
        \begin{itemize}
          \item predict a word based on context.
          \item do not want to predict the prob. of a string (sentence)
          \item not a generative model of a text: independent senses?????
        \end{itemize}
      \item The weights for different positions are \textbf{shared}
      \item computationally much more efficient than normal NNLM
      \item The hidden layer is just linear
    \end{itemize}
  \item Skip-gram: 
    \begin{itemize}
      \item reformulate the CBOW model (opposite to CBOW) by predicting surrounding words (context words, a word nearby) using the current word.
      \item Find word representations usful for predicting surrounding words in a sentence or document.
      \item each word have two vector representations associated with it: \textit{word representation vector} and \textit{output weight vector}
      \item Learning skip-gram: \textit{see below}
    \end{itemize}

\begin{figure}[ht]
  \centering
    \subfigure[CBOW architecture]{\label{fig:cbow}\includegraphics[width=0.3\linewidth]{figures/cbow.png}}
    \subfigure[The Skip-gram model architecture]{\label{fig:skip}\includegraphics[width=0.3\linewidth]{figures/skip.png}}
  \caption{Model}
  \label{fig:wr}
\end{figure}

\end{itemize}

\subsubsection*{Sometimes called unsupervised, but objective is supervised!}
\begin{itemize}
  \item the ideal context word is given, we don't care about supervision
\end{itemize}

% \subsubsection*{Vocabulary is not finite.-- problem for word2vec}
% \begin{itemize}
%   \item fixed number of word, fixed vocab size
%   \item subword representation, character-level
% \end{itemize}

\subsubsection*{Compositional representations based on morphemes make our models closer to open vocabulary.}
\begin{itemize}
  \item more generalizable, predict (represent) unseen words better

\end{itemize}

\subsubsection*{Limitations of word2vec}
\begin{itemize}
  \item fixed number of word, fixed vocab size, but \textbf{Vocabulary is not finite}.
  \item cannot exploit \textit{functional relationships} (syntactic relationship): 

  e.g, we could learn ``cat''-``cats'' = ``dog''-``dogs'', but if we haven't seen ``sloths'' before, we can't capture ``sloth''-``sloths'' = ``cat''-``cats''. That is, we can't learn \textit{functional relationships} from context.
\end{itemize}

\subsubsection*{Learning skip-gram}
\begin{itemize}
  \item SGD and BP
  \item sub-sample the frequent words (e.g., ``the'', ``is'', ``a'')
  \item words are thrown out proportinal to their frequency (make things faster, reuses importance of frequent words)
  \item non-linearity does not seem to imporve performance of these models, thus the hidden layer dose \textbf{not} use \textbf{activation function}
  \item problem: very large output layer with size = vocab. size, can easily be in order of millions $\rightarrow$ too many outputs to evaluate
  \item solution: negative sampling, Hierarchical softmax
\end{itemize}

\subsubsection*{Negative sampling}
\begin{itemize}
  \item Instead of propagating signal from the hidden layer to the whole output layer, only the output neuron that represents the positive class and few randomly sampled neurons are evaluated
  \item e.g., word pair `cat sat', input `cat', context word (output is 1) `sat', output neuron correspond to `sat' is positive, others negative. we sample other, say 5, negative neuron to evaluate. so only calculate 1+5 = 6 neurons. For word representation vector, we \textbf{always only} update the one corresponding to the input word (it's true whenever we use negative sampling or not).
\end{itemize} -->

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ML/" rel="tag"># ML</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/01/09/2019-01-09%20Gaussian%20Process/" rel="prev" title="Gaussian Process 高斯过程">
      <i class="fa fa-chevron-left"></i> Gaussian Process 高斯过程
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#隐变量模型"><span class="nav-number">1.</span> <span class="nav-text">隐变量模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-mean"><span class="nav-number">2.</span> <span class="nav-text">K-mean</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-mean算法是一种隐变量模型"><span class="nav-number">3.</span> <span class="nav-text">K-mean算法是一种隐变量模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-mean算法怎么求"><span class="nav-number">4.</span> <span class="nav-text">K-mean算法怎么求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-mean算法的cost-function"><span class="nav-number">5.</span> <span class="nav-text">K-mean算法的cost function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-mean算法的求解过程，和隐变量模型的EM算法的关系。"><span class="nav-number">6.</span> <span class="nav-text">K-mean算法的求解过程，和隐变量模型的EM算法的关系。</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Xiaoqing Tang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoqing Tang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
